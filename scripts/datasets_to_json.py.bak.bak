import sys
import os.path
import json
import itertools

# Check for required datasets
dataset_base_path = '/home/vagrant/app/ncdc_datasets/raw/'
required_datasets = [
  dataset_base_path+'dly-tmax-normal.txt',
  dataset_base_path+'dly-tmax-stddev.txt',
  dataset_base_path+'dly-tmin-normal.txt',
  dataset_base_path+'dly-tmin-stddev.txt',
  dataset_base_path+'allstations.txt',
  dataset_base_path+'zipcodes-normals-stations.txt'
]
for path in required_datasets:
  if not os.path.isfile(path):
    print path+' is a required dataset. Aborting.'
    sys.exit(1)

# Open datasets
# dly_tmax_normal = open(dataset_base_path+'dly-tmax-normal-test.txt', 'r')
# dly_tmax_stddev = open(dataset_base_path+'dly-tmax-stddev-test.txt', 'r')
dly_tmax_normal = open(dataset_base_path+'dly-tmax-normal.txt', 'r')
dly_tmax_stddev = open(dataset_base_path+'dly-tmax-stddev.txt', 'r')
dly_tmin_normal = open(dataset_base_path+'dly-tmin-normal.txt', 'r')
dly_tmin_stddev = open(dataset_base_path+'dly-tmin-stddev.txt', 'r')
datasets = [
  {'dataset': dly_tmax_normal, 'data_index_name': 'dlyTMaxNormal'},
  {'dataset': dly_tmax_stddev, 'data_index_name': 'dlyTMaxStddev'},
  {'dataset': dly_tmin_normal, 'data_index_name': 'dlyTMinNormal'},
  {'dataset': dly_tmin_stddev, 'data_index_name': 'dlyTMinStddev'},
]

all_station_data = {}
# Ensure all data in datasets come in multiples of 12
# (one line per month, 12 months per year)
for dataset in datasets:
  # If line count for given dataset isn't divisible by 12
  if sum(1 for _ in dataset['dataset']) % 12 != 0:
    print "Error:  Data in dataset (below) not a multiple of 12. Aborting"
    print dataset['dataset']
    sys.exit(1)

  # Return file pointer to the beginning of the file
  dataset['dataset'].seek(0)

for dataset in datasets:
  # Parse datasets into JSON-ready Python dictionary
  while True:
    # Grab the next 12 lines
    chunk = list(itertools.islice(dataset['dataset'], 12))
    if not chunk:
      break

    station_id = chunk[0].split()[0]
    data_completeness_flag = chunk[0].split()[3][-1]

    if station_id in all_station_data:
      all_station_data[station_id][dataset['data_index_name']] = {'completeness': data_completeness_flag, 'data': []}
    else:
      all_station_data[station_id] = {'stationId': station_id, dataset['data_index_name']: {'completeness': data_completeness_flag, 'data': []}}

    for month_data in chunk:
      for temp in month_data.split()[2:]:
        # Handle special values by saving them as zero
        if temp in [-9999, -8888, -7777, -6666, -5555]:
          all_station_data[station_id][dataset['data_index_name']]['data'].append(0)
        else:
          all_station_data[station_id][dataset['data_index_name']]['data'].append(float(temp[:-1])/10)

  dataset['dataset'].close()

slice_station_id = slice(0, 11)
slice_latitude = slice(12, 20)
slice_longitude = slice(21, 30)
slice_elevation = slice(31, 37)
slice_state = slice(38, 40)
slice_name = slice(41, 71)
slice_gsn_flag = slice(72, 75)
slice_hcn_flag = slice(76, 79)
slice_wmoid = slice(80, 85)
slice_method = slice(86, 99)
with open(dataset_base_path+'allstations.txt', 'r') as location_dataset:
  for line in location_dataset:
    if line[slice_station_id].strip() in all_station_data:
      all_station_data[line[slice_station_id].strip()]['location'] = {
      #all_station_data[station_id]['location'] = {
        'latitude': float(line[slice_latitude]),
        'longitude': float(line[slice_longitude]),
        'elevation': float(line[slice_elevation]),
        'state': line[slice_state].strip(),
        'name': line[slice_name].strip(),
        'gsn_flag': line[slice_gsn_flag].strip(),
        'hcn_flag': line[slice_hcn_flag].strip(),
        'wmoid': line[slice_wmoid].strip(),
        'method': line[slice_method].strip()
      }

slice_postal_code = slice(12, 17)
slice_city = slice(18, None)
with open(dataset_base_path+'zipcodes-normals-stations.txt', 'r') as location_dataset:
  for line in location_dataset:
    station_id = line[slice_station_id]
    if station_id in all_station_data:
      if all_station_data[station_id]['location']:
        all_station_data[station_id]['location']['postal_code'] = line[slice_postal_code]
        all_station_data[station_id]['location']['city'] = line[slice_city]
      else:
        all_station_data[station_id]['location'] = {
          'postal_code': float(line[slice_postal_code]),
          'city': float(line[slice_city]),
        }

# Dict of Dicts to List of Dicts
with open('../data/stations-temperature-location.json', 'w') as outfile:
  for key in all_station_data:
    json.dump(all_station_data[key], outfile)
    outfile.write('\n')
